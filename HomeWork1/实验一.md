在本次的实验中,需要根据给定的数据建立向量空间模型

将一系列文档在同一向量空间中表示则组成了向量空间模型
在向量空间模型中,每一个文档都是由该文档的文档向量组成,文档向量中的每一个分量代表着词项在文档中的相对重要性

### 在本实验中主要进行了一下步骤:

* Tokenization 将文本中的句子按照一定的规则分隔开
* Stemming 进行一些词干的还原
* stopwords 去除一些停止词 如 a an the 等
* 统计文档中每一个词的出现次数,同时记录每一个出现在那些文档中

* 根据tf-idf 计算每一个词项在文档中的权重,并生成文档的文档向量

### 遇到的困难

1. 在开始进行实验的时候,遇到了编码问题,无法读取文档

解决方案:在读取的时候,忽略错误
```python
        f=open(filename,'r',errors='ignore')
```

2. tokenization问题,没有找到一种很好的方式将单词分开,曾经试过多种方式,在调试的过程中打印中间结果,总是能够找到不少不正常的词汇.

解决方案: 现在采用了两种方法,在 E1.py 中采取的是将所有的标点符号转化为空格,然后使用空格进行划分,在 E1S.py 中,使用的是 nltk 库中的 sent_tokenize 和 word_tokenize进行划分,通过比较发现在使用nltk库的程序产生的词项较少,但是速度较慢

3. 在先前的版本中,使用[[...],[...]]来模拟二维数组
在运行的过程中,遇到了memoryerror 没有解决,后来更改了数据结构,使用 python 中的字典.

字典 diffwords = {word k : set<出现该词的文档的集合> }

字典 matrix = { doc k : { word k : word k在该文档中出现的次数}}

4. 性能问题,由于对python不是很熟悉,写出来的运行效率不是很高,正在努力优化